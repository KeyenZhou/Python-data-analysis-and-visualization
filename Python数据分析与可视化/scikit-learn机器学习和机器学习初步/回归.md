# 回归

# 1. 线性回归

数据下载：  
[promote_sales_data.txt](./数据集/promote_sales_data.txt)  
[anscombe.csv](./数据集/anscombe.csv)  
[customer_value.csv](./数据集/customer_value.csv)  
[user_info.csv](./数据集/user_info.csv)   

线性回归模型是利用线性拟合的方式来探寻数据背后的规律。如下图所示，通过线性回归模型可找出这些样本点隐含的回归曲线，进而通过回归曲线进行一些简单的预测分析或因果关系分析。
![](./数据集/image%20(12).png)
线性回归中，我们根据特征变量来对结果变量进行预测，根据特征变量的个数可将线性回归模型分为一元线性回归和多元线性回归。

## 1.1 一元线性回归

一元线性回归模型也称为简单线性回归模型，其回归线数学表达表达为：
$$ 
\hat{𝑦}=𝑎 \times 𝑥+𝑏 
$$
其中$\hat{y}$为因变量，x为自变量，a表示回归系数，b表示截距，如下图所示。
![](./数据集/image%20(13).png)
其中$y_i$为实际值，$\hat{y}_i$为预测值，一元线性回归的目的就是拟合出一条线来使得预测值和实际值尽可能的接近，如果大部分点都落在拟合出来的线上，那么该线性回归模型则拟合较好。


#### 例：促销额与销量预测
每个销售型公司都有一定的促销费用，促销费用可以带来销售量的显著提升。文件“promote_sales_data.txt”中包含某公司若干次促销活动花费的促销费和销售量，尝试通过一元线性回归。预测当给出一定的促销费用时，会产生多少销售量。


```python
import pandas as pd
from matplotlib import pyplot as plt

promote_sales_data = pd.read_csv('/data/bigfiles/promote_sales_data.txt')
print(promote_sales_data.head())

# 查看训练数据的相关性，确定使用线性回归模型
x = promote_sales_data[['money']]
y = promote_sales_data[['amount']]
plt.scatter(x, y)
plt.show()
```


```python
# 导入线性回归方法库
from sklearn import linear_model

# 实例化线性回归模型对象
model = linear_model.LinearRegression()
# 使用数据集进行模型训练
model.fit(x, y)
# 求出线性回归方程的自变量和截距，model_coef 是回归系数，model_intercept 是截距
model_coef = model.coef_
model_intercept = model.intercept_
print('coef is: ', model_coef)
print('intercept is: ', model_intercept)
print(f'一元线性回归曲线为：y={model_coef[0, 0]}x+{model_intercept[0]}')
```


```python
# 使用模型，求所有预测值，并绘制回归线
predict_y = model.predict(x)
plt.scatter(x, y)   # 绘制原始数据散点
plt.plot(x, predict_y, color='r')  # 绘制回归线
# 预测促销费用为38500的销售量
new_x = pd.DataFrame([[38500]],columns=['money'])
new_y = model.predict(new_x)
print(new_y)
plt.scatter(new_x.iloc[0, 0], new_y[0, 0], color='m', s=200)   # 绘制促销费用为38500的预测值点
plt.show()
```

## 1.2 一元多次线性回归
一元线性回归模型还有一个进阶版本，叫作一元多次线性回归模型，比较常见的有一元二次线性回归模型，其回归线数学表达式为：
$$\hat{y}=a \times x^2+b \times x+c$$
以“Anscombe四重奏”数据集中的第二类数据为例，对其作一元线性回归。


```python
import pandas as pd
from matplotlib import pyplot as plt
from sklearn import linear_model

anscombe_data = pd.read_csv('/data/bigfiles/anscombe.csv')
anscombe_ii_data = anscombe_data[anscombe_data['dataset']=='II'].sort_values(by='x')
print(anscombe_ii_data.head())

# 查看训练数据的相关性
x_1 = anscombe_ii_data[['x']]
y_1 = anscombe_ii_data[['y']]
# 实例化线性回归模型对象
model_1 = linear_model.LinearRegression()
# 一元线性回归，使用数据集进行模型训练
model_1.fit(x_1, y_1)
plt.plot(x_1, model_1.predict(x_1), color='r')  # 绘制回归线
plt.scatter(x_1, y_1) # 绘制原始数据散点
plt.show()
```

从上图可以明显看出，相比于当前的回归线，用一元二次线性回归模型形成的曲线将更契合散点图背后的趋势。

如需使用一元二次线性回归模型，首先需要对原有的特征数量进行扩充，可使用scikit-learn库提供的数据预处理方法快速生成多项式特征。


```python
# 通过如下代码生成二次项数据：
from sklearn.preprocessing import PolynomialFeatures
poly_reg = PolynomialFeatures(degree=2) # degree=2，将生成[[1, a, a^2]]
x_m = poly_reg.fit_transform(x_1)
print(x_m[:5])
```


```python
# 实例化线性回归模型对象
model_2 = linear_model.LinearRegression()
# 一元二次线性回归，使用数据集进行模型训练
model_2.fit(x_m, y_1)
plt.plot(x_1, model_2.predict(x_m), color='r')  # 绘制回归线
plt.scatter(x_1, y_1) # 绘制原始数据散点
plt.show()
```


```python
# 求出线性回归方程的系数和截距，model_coef 是回归系数，model_intercept 是截距
model_coef = model_2.coef_
model_intercept = model_2.intercept_
print('coef is: ', model_coef)
print('intercept is: ', model_intercept)
```

此时的model_coef中为3个数，第一个0对应之前生成的x_m常数项前面的系数，可见x_m的常数项不会产生影响；2.78083916为x_m一次项的系数，即系数b；-0.12671329为x_m二次项的系数，即系数a；而model_intercept的值-5.99573427则代表常数项c。所以该一元二次线性回归方程为：
$$y=-0.12671329x^2+2.78083916x-5.99573427$$

需要注意的时，应避免引入过多特征导致过拟合。所谓过拟合，是指模型在训练样本中拟合程度过高，虽然它很好地贴合了训练集数据，但是却丧失了泛化能力，模型不具有推广性导致在新的数据集中表现不佳。
过拟合相对的则是欠拟合，欠拟合是指模型拟合程度不高，数据距离拟合曲线较远，或指模型没有很好地捕捉到数据特征，不能够很好地拟合数据。
![](./数据集/image%20(14).png)

模型搭建完成后，我们还需要对模型进行评估，一般以两个值作为评判标准：
* 决定系数（$R^2$）：衡量线性拟合的拟合程度，取值范围为0-1，值越大模型的拟合程度越高。
* 均方误差根（MSE）：预测值与真实值的平均平方差，值越小，说明预测模型描述实验数据具有更好的精确度。



```python
# 导入两个检验函数
from sklearn.metrics import mean_squared_error, r2_score

# 分别输出上面三个模型的评价指标
print('model:')
print(f'MSE: {mean_squared_error(y, model.predict(x)):.0f}')
print(f'R2: {r2_score(y, model.predict(x)):.2f}')
print('model_1:')
print(f'MSE: {mean_squared_error(y_1, model_1.predict(x_1)):.0f}')
print(f'R2: {r2_score(y_1, model_1.predict(x_1)):.2f}')
print('model_2:')
print(f'MSE: {mean_squared_error(y_1, model_2.predict(x_m)):.0f}')
print(f'R2: {r2_score(y_1, model_2.predict(x_m)):.2f}')
```

## 1.3 多元线性回归

多元线性回归模型的原理和一元线性回归的原理类似，其形式可以用如下公式表达：
$$\hat{y}=k_0+k_1 \times x_1+k_2 \times x_2+k_3 \times x_3+ \dots$$

其中：$x_1$、$x_2$、$x_3$……为不同特征变量，$k_1$、$k_2$、$k_3$……则为这些特征变量前的系数，$k_0$为常数项。

#### 例：银行客户价值预测

文件“customer_value.csv”中包含银行多位客户的相关信息，根据数据构建客户价值(一年里能给银行带来的收益)预测模型。而分析出客户的价值后，可以针对高价值的客户进行区别于普通客户的服务，有助于进一步挖掘这些高价值客户的价值，并提高这些高价值客户的忠诚度。


```python
import pandas as pd

customer_value_data = pd.read_csv('/data/bigfiles/customer_value.csv')
customer_value_data.head() 
```

原数据中学历和性别两个特征为字符串类型，在进行多元线性回归之前，需先将其转化为数值型。


```python
# 性别转换为整数
customer_value_data['性别'] = customer_value_data['性别'].map({'女': 0, '男': 1}).astype(int)
# 学历转换为整数
customer_value_data['学历'] = customer_value_data['学历'].map({'高中': 1, '本科': 2, '研究生': 3}).astype(int)
customer_value_data.head() 
```


```python
from sklearn.linear_model import LinearRegression
# 导入两个检验函数
from sklearn.metrics import mean_squared_error, r2_score

x = customer_value_data[['历史贷款金额', '贷款次数', '学历', '月收入', '性别']]
y = customer_value_data['客户价值']
# 实例化线性回归模型对象
model_3 = LinearRegression()
# 多元线性回归，使用数据集进行模型训练
model_3.fit(x,y)
# 求出线性回归方程的系数和截距，model_coef 是回归系数，model_intercept 是截距
model_coef = model_3.coef_
model_intercept = model_3.intercept_
print('coef is: ', model_coef)
print('intercept is: ', model_intercept)
# 输出模型的评价指标
print(f'MSE: {mean_squared_error(y, model_3.predict(x)):.0f}')
print(f'R2: {r2_score(y, model_3.predict(x)):.2f}')
```

# 2. 逻辑回归

机器学习从统计领域借鉴的一种技术，用于分析二分类或有序的因变量与自变量之间的关系。逻辑回归模型虽然名字中有回归两字，其本质却是分类模型。

逻辑回归算法是一种广义的线性回归分析方法，它在线性回归算法的基础上，利用Sigmoid函数（该函数可将任意值转换到0到1的区间内）对事件发生的概率进行预测。也就是说，在线性回归中可以得到一个预测值，然后将该值通过逻辑函数进行转换，将预测值转为概率值，再根据概率值实现分类。逻辑回归常用于数据挖掘、疾病自动诊断和经济预测等领域。


```python
# 绘制Sigmoid函数
import matplotlib.pyplot as plt
import numpy as np

x = np.linspace(-10, 10)  # 通过linspace()函数生成-10到+10的等差数列，默认50个数
y = 1.0 / (1.0 + np.exp(-x))  # Sigmoid函数计算公式，exp()函数表示指数函数
plt.plot(x,y)  # 画图
plt.xlabel('x',fontsize = 10)
plt.ylabel('y',fontsize = 10, rotation = 0)
plt.title(r'$Sigmoid:f(x)= \frac{1}{1 + e^{-x}}$',fontsize = 15)
plt.show()  
```

#### 例：用户购买行为预测
 文件“user_info.csv”中包含25317个用户的18个属性特征，特征变量无缺失值。构建模型，根据除ID外其他特征的值预测y（用户是否购买）。数据字典如下表。
 
 |属性名|含义|属性名|含义|
 |:-----:|:-----|:-----:|:-----|
 |ID|客户唯一标识|age|客户年龄|
|job|客户的职业|marital|婚姻状况|
|education|受教育水平|default|是否有违约记录|
|balance|每年账户的平均余额|housing|是否有住房贷款|
|loan|是否有个人贷欲|contact|客户联系的沟通方式|
|day|最后一次联系的日期|month|最后一次联系的月份|
|duration|最后一次联系的交流时长|campaign|在木次活动中,与该客户交流过的次数|
|pdays|距离上次活动最后一次联系该客户,过去了多久|previous|在本次活动之前,与该客户交流过的次数|
|poutcome|上一次活动的结果|y|预测客户是否会订购定期存款业务|


```python
import pandas as pd

user_info_data = pd.read_csv('/data/bigfiles/user_info.csv', index_col='ID')
user_info_data 
```

数据集中很多属性为分类特征（如：job、marital	、education等），使用scikit-learn库相关预处理函数将分类特征转化为数值。


```python
from sklearn.preprocessing import LabelEncoder # 用于数据预处理模块的标签编码

col_cate = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']  # 分类特征
# 构建简单循环 对分类变量编码
for k in col_cate:
    # LabelEncode编码方式：能够自动将特征中不同的值对应为不同的数字。优点是简单直接；缺点是不一定有符合实际的数值含义
    le = LabelEncoder()    
    user_info_data[k] = le.fit_transform(user_info_data[k])
user_info_data
```

各属性值之间差距较大，使用scikit-learn库相关预处理函数缩放，进行标准化处理。


```python
from sklearn.preprocessing import scale  # 用于数据预处理模块的缩放器

# 缩放处理，对数据按列属性进行scale处理后，每列的数据均值变成0，标准差变为1。
user_info_data.loc[:, 'age':'poutcome'] = scale(user_info_data.loc[:, 'age':'poutcome'])
user_info_data
```

将数据集中的特征属性和预测结果分离，并划分训练集和测试集。


```python
from sklearn.model_selection import train_test_split  # 数据集分类器，于划分训练集和测试集

x = user_info_data.loc[:, 'age':'poutcome']  # 特征集
y = user_info_data['y']  # 结果集
# 对数据集进行划分，test_size参数设置为0.3，随机选取取其中30%的数据作为测试集，70%为训练集
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)
```

构建逻辑回归模型进行训练，并观察比较测试集预测结果与测试集实际结果。


```python
from sklearn.linear_model import LogisticRegression  # 逻辑回归模型

# 实例化逻辑回归模型，max_iter:设置使求解器收敛所需的最大迭代次数，值越大模型迭代时间越长。
log_model = LogisticRegression(max_iter=10000)  
log_model_fit = log_model.fit(x_train, y_train)  # 将训练集的特征和目标放进去
# 调用模型的predict()方法，对测试集的特征数据进行预测，预测结果为y_pred
y_pred = log_model_fit.predict(x_test)
cmp_df = pd.DataFrame()
cmp_df['y_pred'] = list(y_pred)
cmp_df['y_test'] = list(y_test)
cmp_df.head()
```

也可以采用predict_proba方式来预测每个客户产生0或1行为分别对应的概率.


```python
y_pred_prob = log_model_fit.predict_proba(x_test)  # 对结果的概率进行预测
y_pred_prob = pd.DataFrame(y_pred_prob)  # 转化为DataFrame
y_pred_prob.head()
```

可通过调用accuracy_score()函数或模型自带的score函数获取所有测试集数据的预测准确度。


```python
from sklearn.metrics import accuracy_score # 获取预测准确度

print(accuracy_score(y_test, y_pred))
print(log_model_fit.score(x_test, y_test))
```

还可以通过classification_report()打印查看命中率情况。
* support：该类别在测试数据中的样本总量。
* precision：精度，该类别预测结果中的正确率。
* recall：召回率，该类别在测试集中的所有样本的预测正确率
* f1-score：F1分数，精度和召回率的调和平均。精确度和召回率都高时，F1值也会高。F1值在1时达到最佳值（完美的精确度和召回率），最差为0。
* macro avg：每个类别评估指标未加权的平均值
* weighted avg：每个类别评估指标的加权平均。测试集中样本量大的，重要性更高。


```python
from sklearn.metrics import classification_report  # 评估预测结果

print(classification_report(y_test, y_pred))
```

也可通过函数获取ROC曲线及其下面的面积AUC，其取值范围在0.5和1之间。AUC越接近1.0，检测方法真实性越高；等于0.5时，则真实性最低。


```python
from sklearn.metrics import roc_auc_score, roc_curve
import matplotlib.pyplot as plt

# 绘制ROC曲线
fpr, tpr, thres = roc_curve(y_test, y_pred_prob.iloc[:,1])
plt.plot(fpr, tpr)
plt.title('ROC curve')
plt.xlabel('FPR')
plt.ylabel('TPR')
plt.show()
# 计算AUC
print('AUC:', roc_auc_score(y_test, y_pred_prob.iloc[:,1]))
```

如果把假警报率（fpr）理解为代价的话，那么命中率（tpr）就是收益。所以在相同阈值的情况下，希望假警报率（代价）尽量小的情况下，命中率（收益）尽量的高。反映在图形上就是这个曲线尽可能的陡峭，曲线越靠近左上角说明在同样的阈值条件下，命中率越高，假警报率越小，模型越完善。


还可以通过函数计算KS值。KS指标越大，那么模型的风险区分能力越强。KS值小于0.2，一般认为模型区分能力较弱；KS值在[0.2,0.3]区间内，模型具有一定区分能力；KS值在[0.3,0.5]区间内，模型具有较强的区分能力。但KS值也不是越大越好，如果KS值大于0.75，往往表示模型有异常。实际生产环境中，KS值处于[0.2,0.3]区间内，就已经挺不错了。



```python
print('KS:', max(tpr - fpr))
```
