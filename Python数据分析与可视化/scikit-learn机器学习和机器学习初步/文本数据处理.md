# 1. 文本特征提取

文本分析是机器学习算法的主要应用领域。 但文本原始数据中的符号文字序列不能直接传递给算法，因为大多数算法要求具有固定长度的数值型矩阵特征向量，而不是具有可变长度的原始文本文档。为了解决这个问题，scikit-learn提供了一些实用工具可以用最常见的方式从文本内容中抽取数值特征。

## 1.1 CountVectorizer

BOW（Bag Of Words，词袋）模型最初被用在文本分类中，将文档表示成特征矢量。它的基本思想是假定对于一个文本，忽略其词序和语法、句法，仅仅将其看做是一些词汇的集合，而文本中的每个词汇都是独立的。简单说就是讲每篇文档都看成一个袋子，然后看这个袋子里装的都是些什么词汇，每个词汇出现了多少次，根据高频词汇决定文本的分类。


```python
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer

# 创建语料库
corpus = np.array(['The sun is shining.',
                   'The weather is sweet!',
                   'Today is a nice day',
                   'Today is a nice day,the sun is shining and the weather is sweet.'
                  ])
# 词频统计
count = CountVectorizer()
count_vect = count.fit_transform(corpus)
# 输出词汇表
print(count.vocabulary_)
# 查看词频向量
print(count_vect.toarray()) 
```

词汇表中'the':7 表示the这个单词的词频显示在词频向量中的第8位。

词频向量中索引为7的列为\[1,1,0,2\]，表示'the'这个单词在第一个文档中出现1次，在第二个文档中出现1次，在第三个文档中出现0次，在第四个文档中出现2次。

需要注意的是，所有的单词都是小写；单词长度小于两个字母的，会被剔除掉；标点符号会剔除掉。


```python
count.transform(['Something completely new.']).toarray()
```

在训练语料中没有出现的词在后续调用转化方法时将被完全忽略，因此需要较大的语料库来对模型进行训练。

停用词指没有意义的词，如：“and”, “the”, "him"或者中文中的“的”，“了”，“这”。这些词在表示文本内容时被认为是没有信息的，可以删除它们，以避免它们被理解为预测的信号。但有时类似的词对预测很有用，比如在对写作风格或性格进行分类时。在选择停用词时应当尽可能地谨慎，流行的停止词列表可能包括对某些任务具有高度信息性的词。

CountVectorizer提供了一个参数stop_words，用来传入停用词。如果该参数值为english，使用内置的停用词；如果是一个列表，使用列表中的词当停用词；默认为None，不使用停用词。


```python
# 词频统计，带停用词
count_sw = CountVectorizer(stop_words='english')
count_sw_vect = count_sw.fit_transform(corpus)
# 输出词汇表
print(count_sw.vocabulary_)
# 查看词频向量
print(count_sw_vect.toarray()) 
```

## 1.2 基于TF-IDF加权的词向量化

词频逆反文档频率TF-IDF（即词频TF与逆反文档频率IDF的乘积)的主要思想是，如果某个词或短语在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。

TF词频（Term Frequency），指的是某一个给定的词语在该文本中出现的次数，这个数字通常会被归一化(一般是词频除以文章总词数), 以防止它偏向长的文本。计算公式为：
$$TF_w=\frac{N_w}{N}$$
其中$N_w$是在某一文本中词条w出现的次数，N是该文本总词条数。

IDF逆向文件频率 (Inverse Document Frequency)，反应了一个词在所有文本中出现的频率，如果一个词在很多的文本中出现，那么它的IDF值应该低。而反过来如果一个词在比较少的文本中出现，那么它的IDF值应该高。计算公式为 ：
$$IDF_w=\log (\frac {Y}{1+Y_w})$$
Y是语料库的文档总数，$Y_w$是包含词条w的文档数，分母加一是为了避免w未出现在任何文档中从而导致分母为0的情况。

scikit-learn在两个类中实现了TF-IDF方法：TfidfTransformer和TfidfVectorizer，前者接受CountVectorizer生成的稀疏矩阵并将其变换，后者接受文本数据并完成词袋特征提取与TF-IDF变换。


```python
from sklearn.feature_extraction.text import TfidfTransformer

# 根据带停用词的CountVectorizer结果，计算TF-IDF
tf_idf_t = TfidfTransformer()
tf_idf_t_vect = tf_idf_t.fit_transform(count_sw_vect)
print(tf_idf_t_vect.toarray())  # 查看词频逆反文档频率输出
```


```python
from sklearn.feature_extraction.text import TfidfVectorizer

# 根据语料，计算TF-IDF，带停用词
tf_idf_v = TfidfVectorizer(stop_words='english')
tf_idf_v_vect  = tf_idf_v.fit_transform(corpus)
print(tf_idf_v_vect.toarray())  # 查看词频逆反文档频率输出
```

# 2. 使用scikit-learn进行文本分类

#### 例：垃圾短信识别

文件“message80W1.csv”中包含80万条带标签短信数据，在此数据集基础上构建分类模型，用于垃圾短信识别。


```python
import pandas as pd

# 读取数据
sms_data = pd.read_csv('/data/bigfiles/message80W1.csv', header=None, index_col=0)
# 设置列标签
sms_data.columns = ['label', 'message']
sms_data.head()
```

文件内有两列数据：label为短信标签，0代表正常短信，1代表垃圾短信；message为短信文本。


```python
sms_data['label'].value_counts()
```

文件中共有正常短信720000条，垃圾短信80000条。此处考虑到运行时间，从非垃圾短信和垃圾短息中各随机取1000条，用于构建模型。


```python
# 考虑到运行时间，从非垃圾短信和垃圾短息中各取1000条，并拼接到一起
not_spam_df = sms_data[sms_data['label'] == 0].sample(1000)
spam_df = sms_data[sms_data['label'] == 1].sample(1000)
data = pd.concat([not_spam_df, spam_df], axis=0)
data.head()
```

该数据集为脱敏数据集，其中电话号码、邮件地址、身份证号、卡号、日期等敏感信息均已被替换为字母x，使用前应将其全部去除。
数据集中也可能包含重复数据，需做去重处理。


```python
import re

# 利用正则去除脱敏数据
data['message'] = data['message'].apply(lambda x: re.sub('x', '', x))
# 删除重复数据
data['message'].drop_duplicates(inplace=True)
data.head()
```

本例使用文件“stopword.txt”提供的常用停用词，读取该文件，获取停用词表。


```python
# 获取停用词
stop_words = pd.read_csv('/data/bigfiles/stopword.txt', sep='#######', header=None, engine='python')
stop_words = ['≮', '≯', '≠', '≮', ' ', '会', '月', '日', '–'] + list(stop_words.iloc[:, 0])
for i in stop_words[700:800]:
    print(i, end=' ')
```

中文使用jieba库进行分词，并去除停用词。


```python
import jieba

data['message'] = data['message'].apply(lambda x: jieba.lcut(x)) # 分词
data['message'] = data['message'].apply(lambda x: [i for i in x if i not in stop_words])  # 去除停用词
data.head()
```

划分训练集和测试集，将原始数据按照比例分割为测试集和训练集，测试集占总样本的20%。


```python
from sklearn.model_selection import train_test_split

labels = data['label']
messages = data['message'].apply(lambda x: ' '.join(x))
# 划分训练集和测试集，将原始数据按照比例分割为测试集和训练集，测试集占总样本的20%
messages_train, messages_test, labels_train, labels_test = train_test_split(messages, labels, test_size=0.2)
```

使用scikit-learn库方法将文本向量化。


```python
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer

count = CountVectorizer()
# 训练集TF-IDF向量化
messages_train = count.fit_transform(messages_train)
messages_train = TfidfTransformer().fit_transform(messages_train.toarray()).toarray()
# 使用训练集的词汇表，将测试集TF-IDF向量化
messages_test = CountVectorizer(vocabulary=count.vocabulary_).fit_transform(messages_test)
messages_test = TfidfTransformer().fit_transform(messages_test.toarray()).toarray()
```

构建逻辑回归分类模型进行训练。


```python
from sklearn.linear_model import LogisticRegression  # 逻辑回归
from sklearn.metrics import roc_auc_score  # 准确率评价模型好坏

lr = LogisticRegression()  # 逻辑回归
lr.fit(messages_train, labels_train)  # 训练
pred_lr = lr.predict(messages_test)  # 测试
auc_lr = roc_auc_score(labels_test, pred_lr)  # 评价
print('ACC：', lr.score(messages_test, labels_test))
print('AUC：', auc_lr)
```

用数据集内随机数据，测试垃圾短信识别的准确性。


```python
# 随机取10条数据集内短信
test_data = sms_data.sample(10)
test_messages = test_data['message']
# TF-IDF向量化
test_messages = CountVectorizer(vocabulary=count.vocabulary_).fit_transform(test_messages)
test_messages = TfidfTransformer().fit_transform(test_messages.toarray()).toarray()
test_pred_lr = lr.predict(test_messages)
print('预测结果：', test_pred_lr, sep='\n')
print('实际结果：', test_data['label'].tolist(), sep='\n')
```

用几条真实短信测试其可用性。


```python
import numpy as np

real_messages = pd.DataFrame(np.array(['您好，你出100， 我让你赚1000，一月50000，加微信：245579007亏损包赔',
                                       '抢免单新品直播中！第一批限量春装今晚0点限时95折满200-20 s.tb.cn/Y5.jBInl 回T退订',
                                       '我已经提交了申请，就只要等您同意就行了是吗？',
                                       ]), columns=['message'])['message']
# TF-IDF向量化
real_messages = CountVectorizer(vocabulary=count.vocabulary_).fit_transform(real_messages)
real_messages = TfidfTransformer().fit_transform(real_messages.toarray()).toarray()
real_pred_lr = lr.predict(real_messages)
print('预测结果：', real_pred_lr, sep='\n')
```


```python
import pandas as pd
import numpy as np

data = pd.DataFrame({"A":np.arange(5), "B": list("abcde")})
myList = ('w', 'j')
data.head()

data.loc[(data.A >= 2).values, 'A'] = [1, 2, 3]
print(data)

import pandas as pd
import numpy as np

data = pd.DataFrame({"A":np.arange(5), "B": list("abcde")})
myList = ('w', 'j')
data.head()

data.iloc[(data.A >= 2).values, 0] = [[1],[2], [3]]
print(data)
```


```python

```
