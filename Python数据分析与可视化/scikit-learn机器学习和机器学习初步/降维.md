# 1. 什么是降维

简单的说维度就是描述一个事物所需要的性质的数量。针对每一张数据表，维度指的是样本的数量或特征的数量（一般无特别说明，指的都是特征的数量），除了索引之外，有n个特征是n维；对图像来说，维度就是图像中特征向量的数量。

以分类为例，并不是特征数量越多，分类越快，效果越好。事实上，特征数量超过一定值的时候，分类器的效果反而下降，这就是“维度灾难”。
![](./数据集/image%20(16).png)
降维算法中的降维，指的是降低特征矩阵中特征的数量，目的是为了让算法运算更快，效果更好，更便于数据可视化。主要应用场景可视化处理、自然语言处理、信息检索和计算效率提升等。

降维的方法主要有两种：
* 选择特征：即从原有的特征中挑选出最佳的特征
* 抽取特征：即将数据由高维向低维投影，进行坐标的线性转换。

scikit-learn中降维算法都被包括在模块decomposition中，这个模块本质是一个矩阵分解模块，提供主成分分析（Principal Component Analysis，PCA）、截断奇异值分解（Truncated Singular Value Decomposition，TSVD）语义分析、因子分析（Factor Analysis，FA）、独立成分分析（Independent components analysis，ICA ）、非负矩阵分解（Non-negative Matrix Factorization，NMF）、字典学习等算法。下面只结合例子简单介绍使用较多的主成分分析(PCA)。

# 2. 主成分分析PCA（Principal Component Analysis）

PCA主成分分析为典型的抽取特征的方法，它不仅是对高维数据降维，更重要的是经过降维去除噪声，发现数据中的模式。

PCA主成分分析的基本原理为线性映射，简单的来说就是将高维空间数据投影到低维空间上，将数据的主成分（包含信息量大的维度）保留下来，忽略掉对数据描述不重要的成分。在降维中，PCA使用的信息衡量指标是样本方差，又称可解释性方差。方差越大，特征所带的信息量越多。

![](./数据集/image%20(17).png)


#### 例：降维算法在人脸识别中的应用

olivettifaces人脸数据库是纽约大学组建的一个比较小的人脸数据库。有40个人，每人10张图片，组成一张有400张人脸的大图片。像素灰度范围在\[0,255\]。整张图片大小是1190\*942，20行20列，所以每张照片大小是(1190/20)(942/20)= 57\*47=2679。

人脸识别是基于人的脸部特征信息进行身份识别的一种生物识别技术。该技术蓬勃发展，应用广泛，例如：人脸识别门禁系统，刷脸支付软件，公安人脸识别系统。简单人脸识别的本质其实是根据每张人脸不同像素点的颜色不同来进行数据建模与判断，人脸的每个像素点的颜色都有不同的值，这些值可以组成人脸的特征向量们，不过因为人脸上的像素点过多，所以特征变量过多，因此需要利用PCA进行主成分分析进行数据降维。


```python
from PIL import Image
import matplotlib.pyplot as plt

plt.figure(dpi=300,figsize=(12,4))
plt.axis('off')
img_data = Image.open('/data/bigfiles/olivettifaces.gif')
plt.imshow(img_data)
```

将大图片切分成400张人脸图片，存储在一个形状为(400,2679)的二维数组中，每行对应一张人脸图片。


```python
import numpy as np

# 图像数据转为数组
img_ndarray = np.asarray(img_data, dtype='float64')
# 分离人脸照片，40人每人10张照片，每张照片57*47=2679
faces = np.empty((400, 2679))
for row in range(20):
    for column in range(20):
        faces[row * 20 + column] = np.ndarray.flatten(
            img_ndarray[row * 57:(row + 1) * 57, column * 47:(column + 1) * 47])

# 构建标签数组，同一人的照片使用其序号为标签
label = np.empty(400)
for i in range(40):
    label[i * 10:i * 10 + 10] = i
label = label.astype(np.int8)

# 随机显示一个人的10张脸部照片
d = np.random.randint(1,40)
plt.figure(dpi=300,figsize=(12,4))
for i in range(2):
    for j in range(5):
        plt.subplot(5, 5, i*5+j+1)
        plt.imshow(faces[d*10+i*5+j,:].reshape(57,47),cmap='gray')
        plt.axis('off')
plt.show()
```

对数据进行分割，每个人10张照片中，前8张用做训练，后2张用做测试。


```python
train_data = np.empty((320, 2679))
train_label = np.empty((320,))
test_data = np.empty((80, 2679))
test_label = np.empty((80,))

for i in range(40):
    train_data[i * 8:i * 8 + 8] = faces[i * 10:i * 10 + 8]
    train_label[i * 8:i * 8 + 8] = label[i * 10:i * 10 + 8]
    test_data[i * 2:i * 2 + 2] = faces[i * 10 + 8:i * 10 + 10]
    test_label[i * 2:i * 2 + 2] = label[i * 10 + 8:i * 10 + 10]
```

建立KNN模型，进行分类训练，并观察模型分类效果。


```python
from sklearn.neighbors import KNeighborsClassifier  # K最近邻
from sklearn.metrics import roc_auc_score  # 评估预测结果

knn_model_no_pca = KNeighborsClassifier(weights='distance')    # 实例化KNN模型
knn_model_no_pca.fit(train_data, train_label)        # 模型训练
pred_no_pca = knn_model_no_pca.predict(test_data)  # 模型预测
# 结合结果测试集评价模型效果
print('预测结果：\n', pred_no_pca)
print('ACC：', knn_model_no_pca.score(test_data, test_label))
```

每张图片共有2679列，也即有2679个特征变量，这么多特征变量可能带来过拟合以及提高模型的复杂度等的问题，因此我们需要对特征变量进行PCA主成分分析降维，减少特征数。


```python
from sklearn.decomposition import PCA

pca_model = PCA(n_components=320) # 实例化PCA模型，降到100个特征
pca_model.fit(train_data)         # 模型训练
# 使用模型对训练集和测试集降维
train_data_pca = pca_model.transform(train_data)
test_data_pca = pca_model.transform(test_data)
print(train_data_pca.shape,test_data_pca.shape)
```

使用降维后的数据进行分类训练，并观察模型分类效果。


```python
from sklearn.neighbors import KNeighborsClassifier  # K最近邻
from sklearn.metrics import roc_auc_score  # 评估预测结果

knn_model_pca = KNeighborsClassifier(weights='distance')    # 实例化KNN模型
knn_model_pca.fit(train_data_pca, train_label)        # 模型训练
pred_pca = knn_model_pca.predict(test_data_pca)  # 模型预测
# 结合结果测试集评价模型效果
print('预测结果：\n', pred_pca)
print('ACC：', knn_model_pca.score(test_data_pca, test_label))
```

从上面的代码可以看出，数据在降维后减少了大量的特征（从2679到320），但对最后分类精度并未产生负面影响。这个数据集本身数据量不大，当数据量更大时，利用PCA进行降维会发挥更大的作用。
