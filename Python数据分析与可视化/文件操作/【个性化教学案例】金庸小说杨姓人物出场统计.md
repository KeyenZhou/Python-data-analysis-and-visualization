# 金庸小说杨姓人物出场统计

'/data/bigfiles/'下存有金庸全部小说的文本文件，编码为 utf-8.


```python
import os


def get_filename(path: str) -> list:
    """读指定路径下的全部文件中的内容到一个字符串中，返回字符串"""
    file_ls = os.listdir(path)
    # file_ls = ['侠客行.txt', '射雕英雄传.txt', '鹿鼎记.txt', '越女剑.txt', '雪山飞狐.txt', '连城诀.txt', '飞狐外传.txt', '神雕侠侣.txt', '鸳鸯刀.txt', '天龙八部.txt', '三十三剑客图.txt', '碧血剑.txt', '白马啸西风.txt', '笑傲江湖.txt', '倚天屠龙记.txt', '书剑恩仇录.txt']
    return file_ls


if __name__ == '__main__':
    filepath = '/data/bigfiles/'
    text = get_filename(filepath)
    print(text)
```


```python
import os



def read_file(path: str) -> str:
    """读指定路径下的全部文件中的内容到一个字符串中，返回字符串"""
    file_ls = os.listdir(path)
    # file_ls = ['侠客行.txt', '射雕英雄传.txt', '鹿鼎记.txt', '越女剑.txt', '雪山飞狐.txt', '连城诀.txt', '飞狐外传.txt', '神雕侠侣.txt', '鸳鸯刀.txt', '天龙八部.txt', '三十三剑客图.txt', '碧血剑.txt', '白马啸西风.txt', '笑傲江湖.txt', '倚天屠龙记.txt', '书剑恩仇录.txt']
    txt = ''
    for file in file_ls:
        with open(path + file, 'r', encoding='utf-8') as fr:
            txt = txt + fr.read()
    return txt


if __name__ == '__main__':
    filepath = '/data/bigfiles/'
    text = read_file(filepath)
    print(text[:100])  # 输出前100个字符
```

利用 jieba 对读取的字符串进行分词，统计词频。根据词频进行降序排序，返回二维列表。


```python
import os
import jieba


def read_file(path: str) -> str:
    """读指定路径下的全部文件中的内容到一个字符串中，返回字符串"""
    file_ls = os.listdir(path)
    # file_ls = ['侠客行.txt', '射雕英雄传.txt', '鹿鼎记.txt', '越女剑.txt', '雪山飞狐.txt', '连城诀.txt', '飞狐外传.txt', '神雕侠侣.txt', '鸳鸯刀.txt', '天龙八部.txt', '三十三剑客图.txt', '碧血剑.txt', '白马啸西风.txt', '笑傲江湖.txt', '倚天屠龙记.txt', '书剑恩仇录.txt']
    txt = ''
    for file in file_ls:
        with open(path + file, 'r', encoding='utf-8') as fr:
            txt = txt + fr.read()
    return txt


def cut_txt(txt: str) -> list:
    """用 jieba 对文本进行分词，返回列表"""
    word_ls = jieba.lcut(txt)
    return word_ls


def yang_word(word_ls: list) -> list:
    """统计包含‘杨’和‘阳’的词频，根据词频降序排序，返回二维列表"""
    yang_dic = {}
    for word in word_ls:
        if '杨' in word and len(word) > 1:
        # if ('杨' in word or '阳' in word) and len(word) > 1:
            yang_dic[word] = yang_dic.get(word, 0) + 1
    yang_sort = sorted(yang_dic.items(), key=lambda x: x[1], reverse=True)
    return yang_sort


if __name__ == '__main__':
    filepath = '/data/bigfiles/'
    text = read_file(filepath)
    word_lst = cut_txt(text)
    yang = yang_word(word_lst)
    for y, c in yang[:30]:
        print(y, c)

```

从结果中可以发现，jieba 切分结果并不准确，有较多不是人名的切分结果，需要进一步进行处理。


```python
/data/bigfiles/侠客行.txt
```
