# 鸢尾花数据分析与机器学习

# Iris数据集-探索性数据分析

数据作为表格
基本表是一个二维数据网格，其中行表示数据集的各个元素，列表示与每个元素相关的数量。通常，我们将矩阵的行称为样本，行数称为n_samples，矩阵的列称为特征，列数称为n_features。

特征矩阵
-此表布局清楚地表明，信息可以被视为二维数字阵列或矩阵，称为形状为[n_samples，n_Features]的特征矩阵

目标阵列
-除了特征矩阵X，我们通常还使用标签或目标数组，按照惯例，我们通常将其称为y。目标数组通常是一维的，长度为n_samples，通常包含在NumPy数组或Pandas系列中。


```python
# import load_iris function from datasets module
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt


# save "bunch" object containing iris dataset and iits attributes
iris = load_iris()
type(iris) 
```


```python
#print the iris dataset
# Each row represents the flowers and each column represents the length and width.
print (iris.data)
iris.data.shape
```

机器学习术语

每一行都是一个观察（也称为：样本、示例、实例、记录）

每一列都是一个特征（也称为：预测值、属性、独立变量、输入、回归值、协变量）


```python
# 输出四个特征名

print (iris.feature_names)
```


```python
# print the integers representing the species of each observation
# 打印代表每个观察物种的整数

print (iris.target)
```


```python
# print the encoding scheme for species; 0 = Setosa , 1=Versicolor, 2= virginica
# 打印物种的编码方案；0=Setosa，1=Versicolor，2=virginica

print(iris.target_names)
```

Each value we are predicting is the response (also known as: target, outcome, label, dependent variable)
我们预测的每个值都是响应（也称为：目标、结果、标签、因变量）
Classification is supervised learning in which the response is categorical
分类是有监督的学习，其中反应是分类的
Regression is supervised learning in which the response is ordered and continuous
回归是有监督的学习，其中响应是有序和连续的
Requirements for working with data in scikit-learn
在scikit学习中使用数据的要求

1) Features and response are separate objects
1） 特征和响应是单独的对象

2) Features and response should be numeric
2） 特征和响应应为数字

3)Features and response should be NumPy arrays
3） 特征和响应应该是NumPy数组

4)Features and response should have specific shapes
4） 特征和响应应具有特定形状



```python
# Check the types of the features and response
# 检查特征和响应的类型

type('iris.data')
type('iris.target')
```


```python
# Check the shape of the features 
# 检查特征形状
#(first dimension = (ROWS) ie number of observations, second dimensions = (COLUMNS) ie number of features)
# （第一维度=（ROWS）即观察数量，第二维度=（COLUMNS）即特征数量）

iris.data.shape
```


```python
# Check the sape of the response (single dimension matching the number of observation)
# 检查响应的速度（单个维度与观察次数相匹配）


iris.target.shape
```

## 1. 鸢尾花数据集散点图 Scatter Plot with Iris Dataset


```python
# Extract the values for features and create a list called featuresAll
featuresAll=[]
features = iris.data[: , [0,1,2,3]]
print(features.shape)

# Extract the values for targets
targets = iris.target
targets.reshape(targets.shape[0],-1)
print(targets.shape)

# Every observation gets appended into the list once it is read. For loop is used for iteration process
for observation in features:
    featuresAll.append([observation[0] + observation[1] + observation[2] + observation[3]])
print (featuresAll)
```


```python
# Plotting the Scatter plot
import matplotlib.pyplot as plt

plt.scatter(featuresAll, targets, color='red', alpha =1.0)
plt.rcParams['figure.figsize'] = [10,8]
plt.title('Iris Dataset scatter Plot')
plt.xlabel('Features')
plt.ylabel('Targets')
```

## 1a) Scatter Plot with Iris Dataset (Relationship between Sepal Length and Sepal Width) # Method 1
## 1a）鸢尾花数据集散点图（萼片长度和萼片宽度之间的关系）#方法1


```python
# Finding the relationship between Sepal Length and Sepal width
featuresAll = []
targets = []
for feature in features:
    featuresAll.append(feature[0]) #Sepal length
    targets.append(feature[1]) #sepal width

groups = ('Iris-setosa','Iris-versicolor','Iris-virginica')
colors = ('blue', 'green','red')
data = ((featuresAll[:50], targets[:50]), (featuresAll[50:100], targets[50:100]), 
        (featuresAll[100:150], targets[100:150]))

for item, color, group in zip(data,colors,groups): 
    #item = (featuresAll[:50], targets[:50]), (featuresAll[50:100], targets[50:100]), (featuresAll[100:150], targets[100:150])
    x, y = item
    plt.scatter(x, y,color=color,alpha=1)
    plt.title('Iris Dataset scatter Plot')
plt.xlabel('sepal length')
plt.ylabel('Sepal width')
plt.show()
```

## 1b) Scatter Plot with Iris Dataset (Relationship between Petal Length and Petal Width) # Method 1
## 1b）具有鸢尾花数据集的散点图（花瓣长度和花瓣宽度之间的关系）#方法1


```python
# Finding the relationship between Petal Length and Petal width
featuresAll = []
targets = []
for feature in features:
    featuresAll.append(feature[2]) #Petal length
    targets.append(feature[3]) #Petal width

groups = ('Iris-setosa','Iris-versicolor','Iris-virginica')
colors = ('blue', 'green','red')
data = ((featuresAll[:50], targets[:50]), (featuresAll[50:100], targets[50:100]), 
        (featuresAll[100:150], targets[100:150]))

for item, color, group in zip(data,colors,groups): 
    #item = (featuresAll[:50], targets[:50]), (featuresAll[50:100], targets[50:100]), (featuresAll[100:150], targets[100:150])
    x0, y0 = item
    plt.scatter(x0, y0,color=color,alpha=1)
    plt.title('Iris Dataset scatter Plot')
plt.xlabel('Petal length')
plt.ylabel('Petal width')
plt.show()
```

# 2. K - Nearest Neighbours (KNN) Algorithm
# 2.K近邻算法

klearn.neighbors为基于无监督和有监督邻居的学习方法提供了功能。基于监督邻居的学习有两种类型：具有离散标签的数据分类，以及具有连续标签的数据回归。无监督近邻是许多其他学习方法的基础，特别是流形学习和光谱聚类。

尽管它很简单，但最近邻算法在大量分类和回归问题上取得了成功，包括手写数字或卫星图像场景。作为一种非参数方法，它在决策边界非常不规则的分类情况下通常是成功的。


```python
import pandas as pd


iris = load_iris()

ir = pd.DataFrame(iris.data)
ir.columns = iris.feature_names
ir['CLASS'] = iris.target
ir.head()
```

经典的sklearn.neighbors算法可以处理Numpy数组或scipy。稀疏矩阵作为输入。对于密集矩阵，支持大量可能的距离度量。


```python
from sklearn.neighbors import NearestNeighbors

# nn = NearestNeighbors(5) # The arguements specify to return the Fast 5 most among the dataset 
nn = NearestNeighbors()    # The arguements specify to return the Fast 5 most among the dataset 


nn.fit(iris.data)
```


```python
ir.describe()
```


```python
#creating a test data
import numpy as np


test = np.array([5.4,2,2,2.3])
test1 = test.reshape(1,-1)
test1.shape
```


```python
nn.kneighbors(test1,5)
```


```python
# ir.ix[[98, 93, 57, 60, 79],]
# ix is deprecated. Please use .loc for label based indexing or .iloc for positional indexing


ir.iloc[[98, 93, 57, 60, 79],]
```

## 3. KNeighborsClassifier Algorithm
## 3. KNeighbors分类器算法


```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn import neighbors, datasets

n_neighbors = 15

# we only take the first two features. We could avoid this ugly
# slicing by using a two-dim dataset
X = iris.data[:, :2]
y = iris.target

h = .02  # step size in the mesh

# Create color maps
cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])
cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])



for weights in ['uniform', 'distance']:
    # we create an instance of Neighbours Classifier and fit the data.
    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)
    clf.fit(X, y)

    # Plot the decision boundary. For that, we will assign a color to each
    # point in the mesh [x_min, x_max]x[y_min, y_max].
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

    # Put the result into a color plot
    Z = Z.reshape(xx.shape)
    plt.figure()
    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)

    # Plot also the training points
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,
                edgecolor='k', s=20)
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())
    plt.title("3-Class classification (k = %i, weights = '%s')"
              % (n_neighbors, weights))

plt.show()
```

### KNN 分类器算法工作原理简单解释


```python
from sklearn.neighbors import KNeighborsClassifier


knn = KNeighborsClassifier(n_neighbors=1)
print (knn)

```


```python
import numpy as np

X1 = np.asarray(featuresAll)
X1 = X1.reshape(-1,1)
X1.shape
```


```python
y = iris.target

y.shape
```


```python
knn.fit(X1, y)
```


```python
import numpy as np


print (knn.predict([[6.4]]))
```


```python
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X1, y)

```


```python
print (knn.predict([[3.4]]))
print (knn.predict(np.column_stack([[1.,6.1,3.2,4.2]])))
```

## 4.Linear regression
## 4.线性回归

我们将从最熟悉的线性回归开始，即数据的直线拟合。直线拟合是y=ax+b形式的模型，其中A通常称为斜率，b通常称为截距。


我们可以使用Scikit Learn的LinearRegression估计器来拟合该数据并构建最佳拟合线：


```python
from sklearn.linear_model import LinearRegression

model = LinearRegression(fit_intercept=True)
model
```


```python
import numpy as np

XX = np.asarray(featuresAll)
print(xx)

X2 = XX[:, np.newaxis]
# X2
X2.shape
```


```python
y2 = iris.target
y2.shape
```


```python
model.fit(X2, y2)
```

数据的斜率和截距包含在模型的拟合参数中，在Scikit Learn中，这些参数总是用尾随下划线标记。这里的相关参数是coeff和intercept：


```python
model.coef_
```


```python
model.intercept_
```


```python
Xfit = np.random.randint(8,size=(150))
Xfit.astype(float)
Xfit = Xfit[:, np.newaxis]
Xfit.shape

```


```python
yfit = (model.predict(Xfit))
yfit.shape
```


```python
plt.scatter(X2, y2)
plt.plot(Xfit, yfit)
```

## 回归

在统计建模中，回归分析是一组用于估计变量之间关系的统计过程。当关注因变量和一个或多个自变量（或“预测因子”）之间的关系时，它包括许多建模和分析多个变量的技术。更具体地说，回归分析有助于理解当任何一个自变量发生变化而其他自变量保持不变时，因变量（或“标准变量”）的典型值是如何变化的。

您可以使用一个技巧来使线性回归适应变量之间的非线性关系，即根据基函数转换数据。我们以前在超参数、模型验证和特征工程中使用的多项式回归管道中见过这种情况的一个版本。我们的想法是采用多维线性模型：y=a0+a1x1+a2x2+a3x3+⋯ 从我们的一维输入x建立x1、x2、x3等等。

该多项式投影非常有用，可以使用多项式特性转换器将其内置到Scikit Learn中：


```python
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(150, include_bias=False)
poly.fit_transform(X2)
```


```python
from sklearn.pipeline import make_pipeline

poly_model = make_pipeline(PolynomialFeatures(3),
                           LinearRegression())
poly_model.fit(X2, y2)
yfit = poly_model.predict(Xfit)
```


```python
#O ur linear model, through the use of 3rd-order polynomial basis functions, can provide a fit to this non-linear data
plt.scatter(X2, y2)
plt.plot(Xfit, yfit);
```

## 长度和宽度如何根据物种而变化


```python
import pandas as pd
import matplotlib.pyplot as plt

iris1 = pd.read_csv("/data/bigfiles/Iris.csv") #load the dataset
iris1.head(5)
```

1c）具有Iris数据集的散点图（萼片长度和萼片宽度之间的关系）#方法1


```python
iris1.plot(kind ='scatter', x ='SepalLengthCm', y ='SepalWidthCm')
plt.show()
```

1d）具有Iris数据集的散点图（花瓣长度和花瓣宽度之间的关系）方法1


```python
iris1.plot(kind ='scatter', x ='PetalLengthCm', y ='PetalWidthCm')
plt.show()
```


```python
Iris数据直方图
```


```python
exclude = ['Id']
iris1.loc[:, iris1.columns.difference(exclude)].hist() 
plt.figure(figsize=(15,10))
plt.show()
```

小提琴图


```python
import seaborn as sns

plt.figure(figsize=(15,10))
plt.subplot(2,2,1)
sns.violinplot(x='Species',y='PetalLengthCm',data=iris1)
plt.subplot(2,2,2)
sns.violinplot(x='Species',y='PetalWidthCm',data=iris1)
plt.subplot(2,2,3)
sns.violinplot(x='Species',y='SepalLengthCm',data=iris1)
plt.subplot(2,2,4)
sns.violinplot(x='Species',y='SepalWidthCm',data=iris1)
```

现在，当我们训练任何算法时，特征的数量及其相关性起着重要作用。如果存在特征并且许多特征高度相关，那么训练具有所有特征的算法将降低精度。因此，应仔细选择特征。该数据集的特征较少，但我们仍将看到相关性。

## IRIS相关矩阵


```python
corr = iris1.corr()
corr
```


```python
# import correlation matrix to see parametrs which best correlate each other
# According to the correlation matrix results Petal LengthCm and
#PetalWidthCm have positive correlation which is proved by the scatter plot discussed above

import seaborn as sns
import pandas as pd


corr = iris1.corr()
plt.figure(figsize=(10,8)) 
sns.heatmap(corr, 
            xticklabels=corr.columns.values,
            yticklabels=corr.columns.values,
           cmap='viridis', annot=True)
plt.show()
```

## 监督学习示例：Iris分类


```python
# I prefer to use train_test_split for cross-validation
# This piece will prove us if we have overfitting 
X3 = iris1.iloc[:, 0:5]  
Y3 = iris1['Species']
```

我们希望根据以前从未见过的数据对模型进行评估，因此我们将数据分为训练集和测试集。这可以手动完成，但使用train_test_split实用函数更方便


```python
# from sklearn.cross_validation import train_test_split
# cross_validation模块在0.18版本中被弃用，现在已经被model_selection代替。
# 更改为 
from sklearn.model_selection import  train_test_split



X3_train, X3_test, y_train, y_test = train_test_split(X3, Y3, test_size=0.4, random_state=0)
print(" X3_train",X3_train)
print("X3_test",X3_test)
print("y_train",y_train)
print("y_test",y_test)
```

## 通过整理数据，我们可以按照我们的方法预测标签：


```python
# Train and test model

from sklearn.naive_bayes import GaussianNB

model = GaussianNB()
model = model.fit(X3_train ,y_train)
y_model = model.predict(X3_test)
y_model
```

最后，我们可以使用accuracy_score实用程序来查看匹配其真实值的预测标签的比例：


```python
from sklearn.metrics import accuracy_score

accuracy_score(y_test, y_model)
```

准确率超过96%，我们看到，即使是这种非常幼稚的分类算法，对这个特定的数据集也是有效的！

## 用iris数据学习SciKit中的K均值聚类

k-means聚类旨在将n个观测分为k个聚类，其中每个观测属于具有最近均值的聚类，作为聚类的原型。

k均值算法在未标记的多维数据集中搜索预定数量的聚类。它使用最佳聚类的简单概念来实现这一点：

“簇中心”是属于簇的所有点的算术平均值。每个点都比其他群集中心更靠近自己的群集中心。这两个假设是k均值模型的基础。


```python
from sklearn.cluster import KMeans



km = KMeans(n_clusters=3, max_iter =1000)
km.fit(iris.data)
```


```python
km.cluster_centers_
```


```python
km.labels_
```


```python
iris1[' K Mean predicted label'] = km.labels_
iris1
```


```python
#First, let's generate a two-dimensional dataset containing four distinct blobs. 
#To emphasize that this is an unsupervised algorithm, we will leave the labels out of the visualization.

# from sklearn.datasets.samples_generator import make_blobs
# samples_generator模块在新版本scikit-learn中已被移除。
# samples_generator模块中相应的类/函数直接从sklearn.datasets中导入即可
from sklearn.datasets import make_blobs


X1, y_true = make_blobs(n_samples=300, centers=4,
                       cluster_std=0.60, random_state=0)
plt.scatter(X1[:, 0], X1[:, 1], s=50);
```


```python
# By eye, it is relatively easy to pick out the four clusters. 
# The k-means algorithm does this automatically, and in Scikit-Learn uses the typical estimator API:

from sklearn.cluster import KMeans


kmeans = KMeans(n_clusters=4)
kmeans.fit(X1)
y_kmeans = kmeans.predict(X1)

# Let's visualize the results by plotting the data colored by these labels.
# We will also plot the cluster centers as determined by the k-means estimator:
plt.scatter(X1[:, 0], X1[:, 1], c=y_kmeans, s=50, cmap='viridis')
centers = kmeans.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);
```

## 无监督学习示例：iris维度

作为一个无监督学习问题的例子，让我们来看看如何降低Iris数据的维数，以便更容易地将其可视化。回想一下Iris数据是四维的：每个样本都记录了四个特征。

降维的任务是询问是否存在适当的低维表示，以保留数据的基本特征。通常，降维被用来帮助可视化数据：毕竟，以二维绘制数据比以四维或更高维度绘制数据要容易得多！

主成分分析-PCA是一种快速线性降维技术。


```python
from sklearn.decomposition import PCA  # 1. Choose the model class

model = PCA(n_components=2)  # 2. Instantiate the model with hyperparameters
model.fit(X3) 
```


```python
X_2D = model.transform(X3) # 3. Fit to data. Notice y is not specified!
X_2D
```


```python
X_2D.shape # 4. Transform the data to two dimensions
```


```python
X_2D[:, 0]
```


```python
X_2D[:, 1]
```


```python
plt.scatter(X[:, 0], X[:, 1], alpha=0.2)
```

## 使用Iris数据集透视数据


```python
import pandas as pd

iris1 = pd.read_csv("/data/bigfiles/Iris.csv") #load the dataset
iris1.head(10)
```

## 最简单的透视表必须有一个dataframe 和一个索引。


```python
pd.pivot_table(iris1,index=["Id"])
```

也可以有多个索引。事实上，大多数pivot_table参数可以通过列表获取多个值。


```python
pd.pivot_table(iris1,index=["Id","Species"])
```

这很有趣，但不是特别有用。我们可能想做的是按物种和ID来查看这一点。通过更改索引就足够容易了。

pd.pivot_table(iris1,index=["Species","Id"])

您可以看到，数据透视表足够聪明，可以开始聚合数据，并汇总萼片长度和花瓣长度及其物种名称。


```python
pd.pivot_table(iris1,index=["Species"],values=["SepalLengthCm","SepalWidthCm"])
```

SepalLength和SepalWidth列自动对数据进行平均，但我们可以进行计数或求和。


```python
pd.pivot_table(iris1,index=["Species"],values=["SepalLengthCm","SepalWidthCm"],aggfunc=np.sum)
```

aggfunc可以获取函数列表。让我们使用numpy-mean函数和len来尝试求平均值，以获得计数。


```python
pd.pivot_table(iris1,index=["Species"],values=["SepalLengthCm","SepalWidthCm"],aggfunc=[np.mean,len])
```


```python
pd.pivot_table(iris1,index=["Species"],values=["SepalLengthCm","SepalWidthCm"],
               columns=["PetalLengthCm"],aggfunc=[np.sum])
```

NaN有点干扰注意力。如果要删除它们，可以使用fill_value将它们设置为0。


```python
pd.pivot_table(iris1,index=["Species"],values=["SepalLengthCm","SepalWidthCm"],
               columns=["PetalLengthCm"],aggfunc=[np.sum],fill_value=0)
```

将“萼片宽度”添加到索引列表中。


```python
pd.pivot_table(iris1,index=["Species","SepalLengthCm","SepalWidthCm","PetalWidthCm"],
               values=["PetalLengthCm"],aggfunc=[np.sum],fill_value=0)

```

例如对于这个数据集，这种表示更有意义。现在，如果我想看一些进一步的总数呢？margins=True对我们来说是这样。


```python
df = pd.pivot_table(iris1,index=["Species","SepalLengthCm","SepalWidthCm","PetalWidthCm"],
               values=["PetalLengthCm"],aggfunc=[np.sum,np.mean],fill_value=0,margins=True)
df
```

假设，如果你只想看一个物种：


```python
df.query('Species == ["Iris-virginica"]')
```


```python
https://www.kaggle.com/code/lalitharajesh/iris-dataset-exploratory-data-analysis/notebook
```
