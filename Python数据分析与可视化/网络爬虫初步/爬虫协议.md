# 8. 爬虫协议

互联网上的网页通过超级链接互相关联起来形成网状结构。网络爬虫的工作方式就像蜘蛛在网上沿着链接爬来爬去，根据爬取初始URL，解析html网页，抽取其中的超级链接，再接着抓取这些新发现的链接指向的网页，循环往复，从而达到获取网页上的数据的目的。  
在互联网世界中，每天都有不计其数的爬虫在日夜不息地爬取数据，在这个过程中，因WEB服务器默认接受所有访问，网络爬虫可能会给WEB服务器带来巨大的资源的开销，过多的爬虫还会占用带宽资源、甚至导致服务器宕机。服务器上的数据有产权归属，网络爬虫获取数据后牟利将会带来法律的风险。网络爬虫可能具备突破简单访问的控制能力，获取被保护的数据，从而泄露个人隐私。
针对这些网络爬虫可能会引发的问题，网站可以对网络爬虫做一些限制。一是来源审查，通过判断User-Agent进行限制，检查来访者HTTP协议头的User-Agent域，只响应浏览器或友好爬虫的访问；二是发布公告，利用 Robots协议， 告知所有的爬虫网站的爬虫策略，要求爬虫遵守。  

## 8.1 Robots协议

Robots协议（也称为爬虫协议、机器人协议等）的全称是“网络爬虫排除标准”（Robots ExclusionProtocol），网站通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取。网站内容的所有者是网站管理员，爬虫应该尊重所有者的意愿，Robots协议提供了一种网站和爬虫进行沟通的途径，根据协议，网站管理员在网站域名的根目录下放一个robots.txt 文本文件，里面可以指定不同的网络爬虫能访问的页面和禁止访问的页面，指定的页面由正则表达式表示。网络爬虫在采集这个网站之前，首先获取到这个文件，然后解析到其中的规则，然后根据规则来采集网站的数据。这个协议的存在是提醒网络爬虫去遵守一些约定，但起不到防止爬虫的作用。遵守Robots协议的爬虫才是好爬虫，但是并不是每个爬虫都会主动遵守Robots协议。  
当一个爬虫访问一个站点时，首先应该检查该站点根目录下是否存在robots.txt，如果存在，就要按照该文件中的内容来确定访问的范围；如果该文件不存在，将能够访问网站上所有没有被口令保护的页面。  

## 8.2 Robots协议的写法

最简单的robots.txt只有两条规则：  
User-agent：指定对哪些爬虫生效  
Disallow：指定要屏蔽的网址  
爬虫抓取时利用http协议里的User-agent声明自己的身份，robots.txt利用User-agent来区分各个引擎的爬虫，比如说google网页搜索爬虫的User-agent为Googlebot，百度的网页搜索的User-agent为Baiduspider。  

例如淘宝网站不希望被百度抓取，其 https://www.taobao.com/robots.txt 内容设置如下，其中*代表所有，/代表根目录。


```python
User-agent: Baiduspider
Disallow: /

User-agent: baiduspider
Disallow: /
```

京东网站为避免商品信息被竞争对手爬取利用，其Robots协议中明确禁止一淘、购物党、我查查等网站的爬虫：


```python
User-agent: * 
Disallow: /?* 
Disallow: /pop/*.html 
Disallow: /pinpai/*.html?* 
User-agent: EtaoSpider 
Disallow: / 
User-agent: HuihuiSpider 
Disallow: / 
User-agent: GwdangSpider 
Disallow: / 
User-agent: WochachaSpider 
Disallow: /
```
